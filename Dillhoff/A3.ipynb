{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c7f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "%pip install PySide2 -qq\n",
    "from PySide2 import QtCore, QtWidgets, QtGui\n",
    "%pip install scikit-video -qq\n",
    "from skvideo.io import vread\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53c4c5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--num_frames n] [--grey True/False] PATH_TO_VIDEO\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3377: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "class MotionDetector:\n",
    "    def __init__(self,          \n",
    "                 frames,        # The Video.\n",
    "                 hysteresis,    # Frame Hystersis for determining active or inactive objects.\n",
    "                 Motion_threshold,    # The motion threshold for filtering out noise.\n",
    "                 Distance_threshold,    # A distance threshold to determine if an object candidate belongs to an object currently being tracked.\n",
    "                 skip,          # The number of frames to skip between detections. The tracker will still work well even if it is not updated every frame.\n",
    "                 maxobj):         # The number of maximum objects to detect.\n",
    "        pass\n",
    "\n",
    "class KalmanFilter:\n",
    "    def predict():\n",
    "        pass\n",
    "\n",
    "    def update(y):\n",
    "        pass\n",
    "\n",
    "class QtDemo(QtWidgets.QWidget):\n",
    "    def __init__(self, frames):\n",
    "        super().__init__()\n",
    "\n",
    "        self.frames = frames\n",
    "\n",
    "        self.current_frame = 0\n",
    "\n",
    "        self.button = QtWidgets.QPushButton(\"Next Frame\")\n",
    "\n",
    "        # Configure image label\n",
    "        self.img_label = QtWidgets.QLabel(alignment=QtCore.Qt.AlignCenter)\n",
    "        h, w, c = self.frames[0].shape\n",
    "        if c == 1:\n",
    "            img = QtGui.QImage(self.frames[0], w, h, QtGui.QImage.Format_Grayscale8)\n",
    "        else:\n",
    "            img = QtGui.QImage(self.frames[0], w, h, QtGui.QImage.Format_RGB888)\n",
    "        self.img_label.setPixmap(QtGui.QPixmap.fromImage(img))\n",
    "\n",
    "        # Configure slider\n",
    "        self.frame_slider = QtWidgets.QSlider(QtCore.Qt.Orientation.Horizontal)\n",
    "        self.frame_slider.setTickInterval(1)\n",
    "        self.frame_slider.setMinimum(0)\n",
    "        self.frame_slider.setMaximum(self.frames.shape[0]-1)\n",
    "\n",
    "        self.layout = QtWidgets.QVBoxLayout(self)\n",
    "        self.layout.addWidget(self.img_label)\n",
    "        self.layout.addWidget(self.button)\n",
    "        self.layout.addWidget(self.frame_slider)\n",
    "\n",
    "        # Connect functions\n",
    "        self.button.clicked.connect(self.on_click)\n",
    "        self.frame_slider.sliderMoved.connect(self.on_move)\n",
    "\n",
    "    @QtCore.Slot()\n",
    "    def on_click(self):\n",
    "        if self.current_frame == self.frames.shape[0]-1:\n",
    "            return\n",
    "        h, w, c = self.frames[self.current_frame].shape\n",
    "        if c == 1:\n",
    "            img = QtGui.QImage(self.frames[self.current_frame], w, h, QtGui.QImage.Format_Grayscale8)\n",
    "        else:\n",
    "            img = QtGui.QImage(self.frames[self.current_frame], w, h, QtGui.QImage.Format_RGB888)\n",
    "        self.img_label.setPixmap(QtGui.QPixmap.fromImage(img))\n",
    "        self.current_frame += 1\n",
    "\n",
    "    @QtCore.Slot()\n",
    "    def on_move(self, pos):\n",
    "        self.current_frame = pos\n",
    "        h, w, c = self.frames[self.current_frame].shape\n",
    "        if c == 1:\n",
    "            img = QtGui.QImage(self.frames[self.current_frame], w, h, QtGui.QImage.Format_Grayscale8)\n",
    "        else:\n",
    "            img = QtGui.QImage(self.frames[self.current_frame], w, h, QtGui.QImage.Format_RGB888)\n",
    "        self.img_label.setPixmap(QtGui.QPixmap.fromImage(img))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Demo for loading video with Qt5.\")\n",
    "    parser.add_argument(\"video_path\", metavar='PATH_TO_VIDEO', type=str)\n",
    "    parser.add_argument(\"--num_frames\", metavar='n', type=int, default=-1)\n",
    "    parser.add_argument(\"--grey\", metavar='True/False', type=str, default=False)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    num_frames = args.num_frames\n",
    "\n",
    "    if num_frames > 0:\n",
    "        frames = vread(args.video_path, num_frames=num_frames, as_grey=args.grey)\n",
    "    else:\n",
    "        frames = vread(args.video_path, as_grey=args.grey)\n",
    "\n",
    "    app = QtWidgets.QApplication([])\n",
    "\n",
    "    widget = QtDemo(frames)\n",
    "    widget.resize(800, 600)\n",
    "    widget.show()\n",
    "\n",
    "    sys.exit(app.exec_())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beda5f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for ShiTomasi corner detection (good features to track paper)\n",
    "corner_track_params = dict(maxCorners = 10,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a2e997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for lucas kanade optical flow\n",
    "lk_params = dict( winSize  = (200,200),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10,0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "489f6a47",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros_like(prev_frame)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     18\u001b[0m     \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Grab current frame\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     ret,frame \u001b[38;5;241m=\u001b[39m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Grab gray scale\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     frame_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Capture the video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Grab the very first frame of the stream\n",
    "ret, prev_frame = cap.read()\n",
    "\n",
    "# Grab a grayscale image (We will refer to this as the previous frame)\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Grabbing the corners\n",
    "prevPts = cv2.goodFeaturesToTrack(prev_gray, mask = None, **corner_track_params)\n",
    "\n",
    "# Create a matching mask of the previous frame for drawing on later\n",
    "mask = np.zeros_like(prev_frame)\n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Grab current frame\n",
    "    ret,frame = cap.read()\n",
    "    \n",
    "    # Grab gray scale\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the Optical Flow on the Gray Scale Frame\n",
    "    nextPts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, prevPts, None, **lk_params)\n",
    "    \n",
    "    # Using the returned status array (the status output)\n",
    "    # status output status vector (of unsigned chars); each element of the vector is set to 1 if\n",
    "    # the flow for the corresponding features has been found, otherwise, it is set to 0.\n",
    "    good_new = nextPts[status==1]\n",
    "    good_prev = prevPts[status==1]\n",
    "    \n",
    "    # Use ravel to get points to draw lines and circles\n",
    "    for i,(new,prev) in enumerate(zip(good_new,good_prev)):\n",
    "        \n",
    "        x_new,y_new = new.ravel()\n",
    "        x_prev,y_prev = prev.ravel()\n",
    "        \n",
    "#         # Lines will be drawn using the mask created from the first frame\n",
    "#         mask = cv2.line(mask, (x_new,y_new),(x_prev,y_prev), (0,255,0), 3)\n",
    "        \n",
    "#         # Draw red circles at corner points\n",
    "#         frame = cv2.circle(frame,(x_new,y_new),8,(0,0,255),-1)\n",
    "    \n",
    "#     # Display the image along with the mask we drew the line on.\n",
    "#     img = cv2.add(frame,mask)\n",
    "#     cv2.imshow('frame',img)\n",
    "    \n",
    "#     k = cv2.waitKey(30) & 0xff\n",
    "#     if k == 27:\n",
    "#         break\n",
    "   \n",
    "#     # Now update the previous frame and previous points\n",
    "#     prev_gray = frame_gray.copy()\n",
    "#     prevPts = good_new.reshape(-1,1,2)\n",
    "    \n",
    "    \n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f246463a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "\n",
    "# Capture the frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "# Get gray scale image of first frame and make a mask in HSV color\n",
    "prvsImg = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv_mask = np.zeros_like(frame1)\n",
    "hsv_mask[:,:,1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "    nextImg = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Check out the markdown text above for a break down of these paramters, most of these are just suggested defaults\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvsImg,nextImg, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    \n",
    "    \n",
    "    # Color the channels based on the angle of travel\n",
    "    # Pay close attention to your video, the path of the direction of flow will determine color!\n",
    "    mag, ang = cv2.cartToPolar(flow[:,:,0], flow[:,:,1],angleInDegrees=True)\n",
    "    hsv_mask[:,:,0] = ang/2\n",
    "    hsv_mask[:,:,2] = cv2.normalize(mag,None,0,255,cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Convert back to BGR to show with imshow from cv\n",
    "    bgr = cv2.cvtColor(hsv_mask,cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow('frame2',bgr)\n",
    "    \n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == 27:\n",
    "        break\n",
    "    \n",
    "    # Set the Previous image as the next iamge for the loop\n",
    "    prvsImg = nextImg\n",
    "\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f80f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
